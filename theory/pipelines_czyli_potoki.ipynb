{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teoria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Czym są pipeline'y:\n",
    "```\n",
    "\"The pipeline is a Python scikit-learn utility for orchestrating machine learning operations. \n",
    "Pipelines function by allowing a linear series of data transforms to be linked together, resulting in a measurable modeling process\"\n",
    "```\n",
    "W uczeniu maszynowym bardzo rzadko zdarza się, by dane które posiadamy od razu nadawały się do przekazania do modelu ML.\n",
    "Przed tym przeważnie niezbędne są różne przekształcenia jak np.:\n",
    "- podmiana kolumn tekstowych na binarne/\"gorącojedynkowe\" \n",
    "- wypełnienie nulli w numerycznych biorąc mediany\n",
    "- usunięcie części wierszy\n",
    "- normalizacja\n",
    "...\n",
    "\n",
    "Wszystkie te kroki wchodzą w proces znany jako ETL (Extract-Transform-Load) i mogą być wykonywane wywołując poszczególne bloki kodu manualnie, jeden po drugim.\n",
    "\n",
    "Pipeliny są mechanizmem który automatyzuje ten proces\n",
    "- zapewniając pewną standaryzację implementacji \n",
    "- zajmując się przekazaniem danych z jednego wyjścia jednego kroku na wejście kolejnego."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estymatory, Transformatory, Predyktory - pipeline'owy słowniczek\n",
    "Estymatory, transformatory i predyktory to zbiór ustandaryzowanych funkcji stanowiących jakąś część etapu przygotowania danych/modelu i wykonania predycji\n",
    "\n",
    "- __Transformatory (transformers)__ - najczęściej pierwszy etap przygotowania danych - transformatory przekształcają dane w różny sposób. Mogą zmieniać ich rozmiar, usuwać błędy lub ujednolicać wartości. Ich głównym celem jest przygotowanie danych wejściowych dla kolejnych etapów. Proces transformacji jest przeprowadzany za pomocą metody transform() a zostaje zwrócony przekształcony zbiór danych. Przykłady: StandardScaler(), PCA() <br>\n",
    "NOTKA: Transformatory, są specjalnym typem estymatora i dlatego potrzebują zdefiniowania funkcji .fit() choć sama funkcja nie musi nic robić\n",
    "\n",
    "- __Estymatory (estimators) - funkcje oszacowujące__ - Każdy obiekt zdolny do szacowania pewnych parametrów na podstawie zbioru danych jest zwany estymatorem. Może to być zarówno proste działanie, jak np. imputer wyliczający medianą z kolumny, ale i model klasyfikacyjny. Estymatory uczą się na danych treningowych i tworzą model, który będzie używany do przewidywania na danych testowych. Przykłady: inputer, DecisionTreeClassifier\n",
    "\n",
    "- __Predyktory (predictors) - funkcje prognostyczne__ - szczególny rodzaj estymatora, który pozwala na przewidywanie jakichś wartości. Predyktory wymagają wytrenowania modelu na danych treningowych (metodą fit()) a później używają tak wytrenowanego modelu do przewidywania wyników (metodą predict()) Przykłady: LinearRegression, LogisticRegression, KNeighborsClassifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Czy pipeline'y są zawsze potrzebne?\n",
    "Nie - choć używanie ich wygląda profesjonalnie i dla niektórych może wejść w nawyk to czasami może stanowić narzędzie, którego implementacja (szczgólnie dla mniej doświadczonych z pipeline'ami użytkowników) jedynie wydłuży proces przygotowania i wdrożenia modelu.\n",
    "\n",
    "#### Kiedy nie warto?\n",
    "- __Małe i proste zadania__ : W przypadku bardzo prostych i małych zadań, które składają się tylko z jednego lub dwóch etapów przetwarzania danych, używanie potoków może być zbędne i zwiększać złożoność kodu\n",
    "- __Jednorazowe użycie modelu__ - przy implementacji modelu, który będzie miał jednorazowo spełnić jakieś zadanie ale nie będzie wielokrotnie używany może być szybsze uzyskanie wyniku bez dodatkowej implementacji pipeline'a\n",
    "- __Potrzeba podglądania danych na różnych etapach__ - kiedy chcemy mieć łatwą możliwość podejrzenia stanu datafrema'u po każdym kroku (w szczególności gdy jesteśmy poczatkujący i nie jesteśmy całkowicie pewni jak wyglądają nasze dane po każdej transformacji)\n",
    "- __Potrzeba bardziej elastycznego podejścia__ - W przypadku, gdy potrzebujesz bardziej elastycznego podejścia do przetwarzania danych, które pozwoli na łatwe dodawanie i usuwanie etapów przetwarzania danych w zależności od potrzeb, bardziej odpowiednim podejściem może być ręczne wywoływanie metod przetwarzania danych w kodzie niż implementowanie obsługi parametrów dla poszczególnych estymatorów.\n",
    "\n",
    "#### Kiedy warto?\n",
    "- __Kiedy będziemy pracować ze strumieniowanymi danymi__ - praca z danymi w formie \"live\" w naturalny sposób będzie wymagała częstego uruchamiania każdego kroku potoku wielokrotnie. Przekazanie danych do potoku usprawnia ten proces i czyni go czytelniejszym\n",
    "- __Kiedy za różne elementy potoku odpowiadają różni ludzie__ - w większych zespołach może zdarzyć się tak, że będziemy odpowiedzialni tylko zaczęść obróbki danych - wtedy użycie potoku poniekąd wymusza utrzymanie spójnego formatu danych co może ułatwić współpracę\n",
    "- __Dużo eksperymentów z hiperparametrami__ - w zależności od sposobu implementacji bez-potoków, używanie ich może okzać się wygodniejsze, jeżeli chcemy przetestowac wiele wariantów modelu bo możliwym będzie skinfigurowanie parametrów potoku w jednym miejscu\n",
    "- __Wymuszenie organizacji kodu__ - choć poszczególne etapy przygotowania danych można samodzielnie obudować w funkcje/metody, potoki w pewnym sensie wymuszają to na nas co może stanowić dobry nawyk, w szczególności dla ludzi nie potrafiących narzucić sobie samodzielnie odpowiedniej dbałości\n",
    "- __Potrzeba większej wydajności★__ - (★nie testowałem) podobno przy odpowiedniem implementacji możliwe jest równoległe przetwarzanie części danych przez różne etapy ujęte w potoku co przyśpiesza całkowity czas przetworzenia danych; ponadto - podobno część transformatorów może mieć lepszą implementację fit_and_transform niż wywoływanie tego oddzielnie. \n",
    "- __Kiedy chcemy, żeby nasz kod wyglądał bardziej profesjonalnie :)__ - pipeline'y zostały stworzone specjalnie do pracy z obróbką danych i ich znajomość oraz umiejętność zastosowania dobrze świadczy o naszym zorientowaniu w branży"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OOP (Programowanie obiektowe) vs Pipelines\n",
    "Jako, że w pythonie \"wszystko jest obiektem\" - tak samo jest z pipeline'ami.<br>\n",
    "W zależności od tego, jak zaimplementujemy nasze metody / klasy - możemy osiągnąć podobny (lub identyczny) efekt jaki zapewniają pipeline'y.\n",
    "\n",
    "Skorzystanie z pakietu sklearn i pipeline'ów pozwala na dopasowanie się do pewnych standardów, może zwiększyć wydajność ale jednocześnie odbiera odrobinę swobody."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sklearn.pipeline.Pipeline vs sklearn.pipeline.make_pipeline \n",
    "__Pipeline__ wymaga definiowania przez nas nazw poszczególnych kroków przy definiowaniu potoku:\n",
    "```\n",
    "pipe = Pipeline([('std_sc', StandardScaler()),\n",
    "                 ('svc_', SVC(gamma='auto'))])\n",
    "```\n",
    "Nadane nazwy kroków: `[std_sc, svc_]`<br><br>\n",
    "\n",
    "__make_pipeline__ to konstruktor Pipeline'ów, który sam nadaje nazwy (biorąc nazwę użytego estyamtora jako lowercase):\n",
    "```\n",
    "mp = make_pipeline(StandardScaler(),\n",
    "                   SVC(gamma='auto')) \n",
    "```\n",
    "Otrzymane nazwy kroków: `[standardscaler, svc]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data ={\n",
    "    \"Name\": [\"Anna\", \"Bob\", \"Charlie\", \"Diana\", \"Eric\", ],\n",
    "    \"Age\": [20, 34, 23, None, 33, ],\n",
    "    \"Gender\": [\"f\", \"m\", \"m\", \"f\", \"m\", ],\n",
    "    \"Job\": [\"Programmer\", \"Writter\", \"Cook\", \"Programmer\", \"Teacher\", ]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Drop name feature\n",
    "df = df.drop(['Name'], axis=1)\n",
    "\n",
    "# Impute ages\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "# Convert gender to numeric\n",
    "gender_dict = {'m': 0, 'f':1}\n",
    "df['Gender'] = [gender_dict[g] for g in df['Gender']]\n",
    "\n",
    "# OneHotEncode Jobs\n",
    "encoder = OneHotEncoder()\n",
    "matrix = encoder.fit_transform(df[['Job']]).toarray()\n",
    "\n",
    "column_names = ['Programmer', 'Writer', 'Cook', 'Teacher']\n",
    "\n",
    "for i in range(len(matrix.T)):\n",
    "    df[column_names[i]] = matrix.T[i]\n",
    "\n",
    "df.drop(['Job'], axis=1)\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2b527f47bf368389ac5a83d7ef3cd7a5f64773e15d0a10dfd3601df79df7542e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
